# SAGA: Strategic Adversarial & Constraint-differential Generative workflow for Test Case Generation

This repository provides supplementary materials for the research paper titled "Rethinking Evaluation for LLM Code Generation: From Generation to Testing," submitted for review. This work introduces **SAGA**, a novel human-LLM collaborative framework for advanced Test Case Generation (TCG).




## üöÄ Introduction to SAGA

Current benchmarks for evaluating Large Language Model (LLM) code generation often use limited or homogeneous test cases, potentially leading to inflated performance metrics and hindering robust model development. SAGA addresses these shortcomings by systematically integrating human programming expertise with LLM reasoning to generate high-quality, diverse, and discriminative test suites. It employs a dual-pronged analytical approach:
*   **Multidimensional Analysis:** Leveraging insights from correct human solutions.
*   **Differential Analysis:** Analyzing incorrect human submissions against corrected versions.

The core objective of SAGA is to maximize both individual test case potency and overall test suite diversity.

## üåü Key Assets Provided

This repository contains the following anonymized assets to support the review process and facilitate understanding of our work:

*   **TCGBench-Lite Dataset:**
    *   Problem descriptions for the 270 problems used in our main experiments.
    *   This dataset is curated from recent competitive programming contests to ensure relevance.
    *   *Location:* `data/tcgbenc_lite_problems.jsonl`(Demo Only)
*   **CodeComPass Verifier Samples:**
    *   A representative sample of the SAGA-generated verifiers for problems in TCGBench-Lite, illustrating the output of our framework.
    *   *Location:* `data/codecompass_verifiers_sample/`
*   **SAGA Prompt Templates:**
    *   Anonymized templates illustrating the structure of prompts used for Multidimensional and Differential Analysis within the SAGA framework. These demonstrate how human insights are structured to guide the LLM.
    *   *Location:* `prompts/`
*   **Demo of SAGA-Generated Test Case:**
    *   An example Python script representing a test case input generated by SAGA, showcasing its structure and complexity.
    *   *Location:* `demos/parse.py`

Further details on the construction and characteristics of TCGBench, TCGBench-Lite, and CodeComPass can be found in the main paper and its appendix.

## üõ†Ô∏è Using the Provided Assets

The provided data and prompts are intended to give reviewers insight into the inputs and outputs of the SAGA methodology and the nature of the datasets used for evaluation.

*   The `tcgbenc_lite_problems.jsonl` file contains problem statements that were used as input for TCG.
*   The `codecompass_verifiers_sample/` directory shows examples of the test suites generated by SAGA.
*   The `prompts/` directory contains templates that illustrate the guidance provided to the LLM within SAGA.
*   The `demos/` directory provides a concrete example of a SAGA-generated test input script.



